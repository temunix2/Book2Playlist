{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import time\n",
    "import pickle\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from gensim import corpora, models, similarities, matutils\n",
    "import gensim\n",
    "from gensim import corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "songs = pd.read_pickle('50k_song.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = stopwords.words('english')\n",
    "stop += ['.', ',', '(', ')', \"'\", \"\\'\", '\"','_',\"oh\", 'ooh', 'ah','ai','na','wan',\"yah\",'na','wan','ai','ca'\\\n",
    "        ,'day','get','got','could','thing','right','let','ta','hey']\n",
    "stop = set(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = []\n",
    "text = list(songs['newtext'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57650"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stem Or Lem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma=nltk.stem.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install textblob\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "lem_text = []\n",
    "count = 0\n",
    "total = 0\n",
    "for num in range(len(text[0:])):\n",
    "    start = time.time()\n",
    "#     print(num)\n",
    "    temp = \"\"\n",
    "    for word in TextBlob(text[num]).words:\n",
    "        temp += lemma.lemmatize(word)\n",
    "        temp += ' '\n",
    "    lem_text.append(temp)\n",
    "    count += 1\n",
    "    end = time.time()\n",
    "    total += end-start\n",
    "    print(f'{count}: {end - start} seconds')\n",
    "    print(f'total seconds: {total}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('song_lemma.pkl', 'wb') as f:\n",
    "    pickle.dump(lem_text, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('song_lemma.pkl', 'rb') as f:\n",
    "    lem_song = pickle.load(f) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.33 s, sys: 85.1 ms, total: 4.42 s\n",
      "Wall time: 4.46 s\n",
      "CPU times: user 4.07 s, sys: 30.9 ms, total: 4.1 s\n",
      "Wall time: 4.12 s\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(stop_words=stop,min_df = 0.02,ngram_range=(1,1))\n",
    "# min_df = 0.0,max_df = 1.0\n",
    "%time vectorizer.fit(lem_song)\n",
    "%time song_ft = vectorizer.transform(lem_song)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.3 s, sys: 137 ms, total: 5.44 s\n",
      "Wall time: 5.51 s\n"
     ]
    }
   ],
   "source": [
    "tfidf_vec = TfidfVectorizer(min_df=0.02,stop_words=stop)\n",
    "%time tfidf = tfidf_vec.fit_transform(lem_song)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15.1 s, sys: 394 ms, total: 15.4 s\n",
      "Wall time: 15.9 s\n",
      "CPU times: user 3.85 ms, sys: 561 Âµs, total: 4.41 ms\n",
      "Wall time: 4.06 ms\n"
     ]
    }
   ],
   "source": [
    "nmf_model = NMF(n_components=20, init='random')\n",
    "%time dtm_nmf = nmf_model.fit_transform(tfidf)\n",
    "%time dtm_nmf = Normalizer(copy=False).fit_transform(dtm_nmf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([feature_names[i]\n",
    "        for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0: like nigga feel bitch shit fuck make look cause money rock em hit big put\n",
      "Topic #1: man woman well said old hand good boy big son money understand god people men\n",
      "Topic #2: night light dream blue sky sing song eye star lord god hear heart sun moon\n",
      "Topic #3: love heart true forever sweet give kiss feel mine hold arm darling fall together lover\n",
      "Topic #4: come true around waiting dance wait along together please may sun see play tomorrow dream\n",
      "Topic #5: yeah alright well cause good everybody yes say said better rock hit big put money\n",
      "Topic #6: life see way world live feel believe look eye find make nothing living inside everything\n",
      "Topic #7: want make cause say tell give really hear anything live take everything see talk think\n",
      "Topic #8: time good mind long mine every last line year remember first think find old keep\n",
      "Topic #9: know tell cause really think well show better yes feel going much something wrong care\n",
      "Topic #10: back home long way coming said well old going gone road town alone bring left\n",
      "Topic #11: away take heart far stay run walk fly gone turn break leave today hand give\n",
      "Topic #12: chorus repeat verse bridge jesus lord god cause name word hand every heart give keep\n",
      "Topic #13: gon tonight make cause better take well rock alright stop keep tell give roll break\n",
      "Topic #14: girl boy world good bad pretty tell woman bout make guy fine young cause hair\n",
      "Topic #15: never say would ever always heart goodbye said tell thought word knew think leave nothing\n",
      "Topic #16: one two another done three together chance else number last care fun many give stand\n",
      "Topic #17: baby please honey babe good crazy cry tonight well make sweet cause tell way night\n",
      "Topic #18: need give someone hold help please tonight call cause somebody everything tell keep much touch\n",
      "Topic #19: little bit boy well said make sweet big daddy mama pretty trouble child every year\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_top_words(nmf_model, vectorizer.get_feature_names(), 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "528"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude = set(string.punctuation)\n",
    "exclude = [i for i in exclude if i != \"'\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma = nltk.stem.WordNetLemmatizer()\n",
    "def clean(doc):\n",
    "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_clean = [clean(doc).split() for doc in songs['newtext']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(doc_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lda = gensim.models.ldamodel.LdaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 13s, sys: 2.19 s, total: 3min 15s\n",
      "Wall time: 3min 21s\n"
     ]
    }
   ],
   "source": [
    "%time ldamodel = Lda(doc_term_matrix, num_topics=20, id2word = dictionary , passes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = ldamodel.print_topics(num_topics=20, num_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
